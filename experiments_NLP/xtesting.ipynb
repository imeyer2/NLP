{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yahoo! Answers Dataset\n",
    "Testing and manipulation\n",
    "\n",
    "https://huggingface.co/datasets/yahoo_answers_topics/blob/main/README.md\n",
    "\n",
    "Original REPO: https://github.com/LC-John/Yahoo-Answers-Topic-Classification-Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS CAN ONLY RUN IN GPU!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "import json\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "import math\n",
    "import hashlib\n",
    "from types import SimpleNamespace\n",
    "from typing import Optional, Union, Tuple\n",
    "import warnings\n",
    "\n",
    "import pynvml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "#QLoRA stuff\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "from pytorch_soo import pytorch_soo\n",
    "from pytorch_soo.pytorch_soo import optim_hfcr_newton\n",
    "\n",
    "from pytorch_soo.pytorch_soo import quasi_newton\n",
    "\n",
    "from pytorch_soo.pytorch_soo import nonlinear_conjugate_gradient as nlcg\n",
    "from pytorch_soo.pytorch_soo.line_search_spec import LineSearchSpec\n",
    "from pytorch_soo.pytorch_soo.trust_region import BadTrustRegionSpec, TrustRegionSpec\n",
    "\n",
    "from yahoo_answers_dataset import Yahoo_Answers_Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size_train:int, batch_size_test:int, device) -> Tuple:\n",
    "    \"\"\"\n",
    "    Get the data based upon the relevant arguments\n",
    "    \"\"\"\n",
    "\n",
    "    train = pd.read_csv(\"/rcfs/projects/sml2024/train_clean_news_articles_categorization.csv\")\n",
    "\n",
    "    train_text = train.iloc[:,-1].to_list()\n",
    "\n",
    "    train_targets = torch.tensor(train.iloc[:,:-1].to_numpy()).to('cpu')\n",
    "    # global NUM_OUTPUTS\n",
    "    # NUM_OUTPUTS = train_targets.shape[1]\n",
    "\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2\", max_length=INPUT_LENGTH)\n",
    "    \n",
    "    # tokenizer.padding_side = \"left\"\n",
    "    # Define PAD Token = EOS Token\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    # tokenizer = transformers.GPT2Tokenizer() #What's the difference betgween this and the one above?\n",
    "\n",
    "\n",
    "    train_tokenizer_output = tokenizer(train_text, return_tensors='pt', padding='max_length',\n",
    "                                       max_length=INPUT_LENGTH, truncation=True).to('cpu')\n",
    "\n",
    "    #New!\n",
    "    train_tokenized_ids = train_tokenizer_output['input_ids']\n",
    "    train_masks = train_tokenizer_output['attention_mask']\n",
    "\n",
    "    #Concatenate the tokenized ids and the masks horizontally to decompose later\n",
    "    train_ids_and_mask = torch.cat(tensors=(train_tokenized_ids, train_masks), dim = 1)\n",
    "\n",
    "    \n",
    "    print(\"The training data has been loaded in\", flush = True)\n",
    "\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(train_ids_and_mask, train_targets)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "    ###############################\n",
    "    ###########Test Data###########\n",
    "    ###############################\n",
    "\n",
    "    test_csv = pd.read_csv(\"/rcfs/projects/sml2024/test_clean_news_articles_categorization.csv\")\n",
    "    test_text = test_csv.iloc[:,-1].to_list()\n",
    "\n",
    "    test_tokenizer_output = tokenizer(test_text, return_tensors='pt', padding='max_length',\n",
    "                                       max_length=INPUT_LENGTH, truncation=True)\n",
    "    # test_tokenized_inputs.to(device)\n",
    "    test_tokenized_ids = test_tokenizer_output['input_ids']\n",
    "    test_masks = test_tokenizer_output['attention_mask']\n",
    "\n",
    "    #Concatenate the tokenized ids and the masks horizontally to decompose later\n",
    "    test_ids_and_mask = torch.cat(tensors=(test_tokenized_ids, test_masks), dim = 1)\n",
    "\n",
    "    #One hot encode the targets\n",
    "    test_targets = torch.tensor(test_csv.iloc[:,:-1].to_numpy())\n",
    "\n",
    "    test = torch.utils.data.TensorDataset(test_ids_and_mask, test_targets)\n",
    "    print(\"The test data has been loaded in\", flush = True)\n",
    "\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size_test, shuffle=False)\n",
    "\n",
    "\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LENGTH = 256\n",
    "NUM_OUTPUTS = 8\n",
    "def get_model_and_loss(device='cpu'):\n",
    "    \"\"\"\n",
    "    Does what it says on the tin.\n",
    "    \"\"\"\n",
    "    compute_dtype = getattr(torch, \"float16\")\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        load_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = transformers.AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path='gpt2',\n",
    "                                                                            max_length=INPUT_LENGTH,\n",
    "                                                                            num_labels=NUM_OUTPUTS,\n",
    "                                                                            quantization_config=bnb_config,\n",
    "                                                                            device_map = device)\n",
    "    # model = transformers.AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=\"distilbert/distilgpt2\", max_length=INPUT_LENGTH, num_labels=NUM_OUTPUTS)\n",
    "    \n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    config = LoraConfig(\n",
    "        r=32, #Higher = more expressivity. Controls number of parameters used i.e. memory\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"score\", \"c_attn\", \"c_proj\", \"c_fc\", \"score.weights\"], #target_modules = [\"c_attn\", \"c_proj\", \"c_fc\"]\n",
    "        bias = \"none\",\n",
    "        lora_dropout=0 #FIXME: Check with SOO paper and see if this needs to be set to 0?\n",
    "        # task_type = \"SEQ_CLS\"\n",
    "    )\n",
    "\n",
    "    # lora_config = LoraConfig(\n",
    "    #     r=16,\n",
    "    #     lora_alpha=32,\n",
    "    #     target_modules=target_modules,\n",
    "    #     lora_dropout=0.05,\n",
    "    #     bias=\"none\",\n",
    "    #     task_type=\"SEQ_CLS\"\n",
    "    #     )\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model = get_peft_model(model, config)\n",
    "    # print(peft.print_number_of_trainable_model_parameters(model))\n",
    "    # model.to('cpu')\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    loss_calc = torch.nn.MSELoss() #Categorical cross entropy doesn't support one hot encoding  MSELoss\n",
    "\n",
    "    # if args.read_nn:\n",
    "    #     print(\"Reading: \", args.read_nn, flush = True)\n",
    "    #     model.load_state_dict(torch.load(args.read_nn))\n",
    "\n",
    "    return model, loss_calc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['load_4bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,743,424 || all params: 129,189,376 || trainable%: 3.671682724127408\n"
     ]
    }
   ],
   "source": [
    "model, loss_calc = get_model_and_loss('cuda')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training data has been loaded in\n",
      "The test data has been loaded in\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_data(32, batch_size_test =32, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for data, targets in train_loader:\n",
    "#     n = data.shape[1]//2\n",
    "#     optim.zero_grad()\n",
    "#     inputs, attention = data[:,:n].to('cuda'), data[:,n:].to('cuda')\n",
    "#     output = model(inputs, attention)\n",
    "#     probs = torch.nn.functional.softmax(output.logits, dim = 1)\n",
    "#     loss = lossfn(probs.to('cpu'), targets.to('cpu'))\n",
    "#     print(f\"loss is {loss}\")\n",
    "#     print(f\"probs are {probs[0]}\")\n",
    "#     model_class_prediction = torch.argmax(probs, dim = 1)\n",
    "#     target_index = torch.argmax(targets, dim = 1)\n",
    "\n",
    "#             #Calculate number correct in this batch\n",
    "#     batch_correct = torch.sum(model_class_prediction.to('cpu')[0] == target_index.to('cpu')[0]).item()\n",
    "#     print(model_class_prediction[0])\n",
    "#     print(target_index[0])\n",
    "#     print(batch_correct)\n",
    "#     optim.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/1835 (0%)]\tLoss: 0.139965\n",
      "Train Epoch: 0 [320/1835 (17%)]\tLoss: 0.137446\n",
      "Train Epoch: 0 [640/1835 (34%)]\tLoss: 0.134003\n",
      "Train Epoch: 0 [960/1835 (52%)]\tLoss: 0.129717\n",
      "Train Epoch: 0 [1280/1835 (69%)]\tLoss: 0.115924\n",
      "Train Epoch: 0 [1600/1835 (86%)]\tLoss: 0.127033\n",
      "\n",
      "Train set: Avg. loss: 7.6295, Accuracy: 234/1835 (12.75%)\n",
      "Train Epoch: 1 [0/1835 (0%)]\tLoss: 0.116311\n",
      "Train Epoch: 1 [320/1835 (17%)]\tLoss: 0.128192\n",
      "Train Epoch: 1 [640/1835 (34%)]\tLoss: 0.120334\n",
      "Train Epoch: 1 [960/1835 (52%)]\tLoss: 0.118774\n",
      "Train Epoch: 1 [1280/1835 (69%)]\tLoss: 0.124137\n",
      "Train Epoch: 1 [1600/1835 (86%)]\tLoss: 0.121284\n",
      "\n",
      "Train set: Avg. loss: 7.3401, Accuracy: 271/1835 (14.77%)\n",
      "Train Epoch: 2 [0/1835 (0%)]\tLoss: 0.143413\n",
      "Train Epoch: 2 [320/1835 (17%)]\tLoss: 0.123008\n",
      "Train Epoch: 2 [640/1835 (34%)]\tLoss: 0.125193\n",
      "Train Epoch: 2 [960/1835 (52%)]\tLoss: 0.125214\n",
      "Train Epoch: 2 [1280/1835 (69%)]\tLoss: 0.122648\n",
      "Train Epoch: 2 [1600/1835 (86%)]\tLoss: 0.127292\n",
      "\n",
      "Train set: Avg. loss: 6.9909, Accuracy: 310/1835 (16.89%)\n",
      "Train Epoch: 3 [0/1835 (0%)]\tLoss: 0.119926\n",
      "Train Epoch: 3 [320/1835 (17%)]\tLoss: 0.115775\n",
      "Train Epoch: 3 [640/1835 (34%)]\tLoss: 0.111271\n",
      "Train Epoch: 3 [960/1835 (52%)]\tLoss: 0.121578\n",
      "Train Epoch: 3 [1280/1835 (69%)]\tLoss: 0.112760\n",
      "Train Epoch: 3 [1600/1835 (86%)]\tLoss: 0.119231\n",
      "\n",
      "Train set: Avg. loss: 6.8217, Accuracy: 314/1835 (17.11%)\n",
      "Train Epoch: 4 [0/1835 (0%)]\tLoss: 0.116047\n",
      "Train Epoch: 4 [320/1835 (17%)]\tLoss: 0.112988\n",
      "Train Epoch: 4 [640/1835 (34%)]\tLoss: 0.113161\n",
      "Train Epoch: 4 [960/1835 (52%)]\tLoss: 0.117941\n",
      "Train Epoch: 4 [1280/1835 (69%)]\tLoss: 0.122057\n",
      "Train Epoch: 4 [1600/1835 (86%)]\tLoss: 0.112985\n",
      "\n",
      "Train set: Avg. loss: 6.6708, Accuracy: 355/1835 (19.35%)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    # epoch = 2\n",
    "    \"\"\"Perform a training epoch using a first order optimizer\"\"\"\n",
    "    model.to('cuda')\n",
    "    model.train()\n",
    "    \n",
    "    linear_probe = False\n",
    "    if linear_probe:\n",
    "        #Freeze everything\n",
    "        for name,param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "            if name == 'score.weight':\n",
    "                param.requires_grad = True\n",
    "            if name == 'score.bias':\n",
    "                param.requires_grad = True\n",
    "        print(\"Using linear probe!\")\n",
    "        \n",
    "    \n",
    "    correct = 0\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    \n",
    "        #Split data into the actual inputs and the mask\n",
    "        n = data.shape[1]\n",
    "        n = n//2\n",
    "        data, attention_mask =  data[:,:n].to('cuda'), data[:,n:].to('cuda')\n",
    "        target.to('cuda')\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids=data, attention_mask=attention_mask)\n",
    "        softmax_out = torch.nn.functional.softmax(output.logits, dim = 1)\n",
    "    \n",
    "        #Next two lines solved a strange device mismatch error.\n",
    "        softmax_out = softmax_out.to('cuda')\n",
    "        target = target.to('cuda')\n",
    "        target = target.to(torch.float32)\n",
    "    \n",
    "        loss = loss_calc(softmax_out, target)  #loss_calc is MSE loss\n",
    "        train_loss += loss.item()\n",
    "        if math.isnan(train_loss):\n",
    "            accuracy = float(\"nan\")\n",
    "            break\n",
    "        \n",
    "        #Returns a tensor, where each entry is the index of the largest element in a row. \n",
    "        model_class_prediction = torch.argmax(softmax_out, dim = 1)\n",
    "        target_index = torch.argmax(target, dim = 1)\n",
    "    \n",
    "        #Calculate number correct in this batch\n",
    "        batch_correct = torch.sum(model_class_prediction == target_index).item()\n",
    "    \n",
    "        correct += batch_correct\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "    print(\n",
    "        \"\\nTrain set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\".format(\n",
    "            train_loss,\n",
    "            correct,\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * correct / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    accuracy = 100.0 * correct / len(train_loader.dataset)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary(abbreviated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# cd existing-project\n",
    "# git init\n",
    "# git add dataset.ipynb\n",
    "# git commit -m \"Initial Commit\"\n",
    "# git remote add origin https://stash.pnnl.gov/scm/~meye795/soo.git\n",
    "# git push -u origin HEAD:main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Type of the training set is {type(ya.train)}\")\n",
    "print(f\"With keys {ya.train.keys()}\")\n",
    "len(ya.train['seq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_config = transformers.PretrainedConfig.from_pretrained()INPUT_LENGTH = 100\n",
    "INPUT_LENGTH = 100\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"openai-community/gpt2\", max_length=INPUT_LENGTH)\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer = transformers.GPT2Tokenizer() #What's the difference betgween this and the one above?\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained('openai-community/gpt2', max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.tokenize() #whats the difference between tokenizer.encode()??\n",
    "\n",
    "#TODO: Add tokenizer config. Cutoff, padding size, unknown token.\n",
    "tokenized_inputs = [tokenizer.encode(x,return_tensors='pt',padding=\"max_length\", max_length=INPUT_LENGTH, truncation = True) for x in ya.train['seq']]\n",
    "tokenized_inputs = torch.vstack(tensors = tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(tokenized_inputs, 'tokenized_inputs.pt')\n",
    "tokenized_inputs = torch.load('tokenized_inputs.pt')\n",
    "tokenized_inputs.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(tokenized_inputs, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the optimizer\n",
    "Based off the paper: https://arxiv.org/pdf/2307.11684.pdf Minibatching Offers Improved Generalization Performance for Second Order Optimizers (ICML 2023)\n",
    "\n",
    "### READ THIS REPO!!! https://github.com/pnnl/pytorch_soo/blob/master/pytorch_soo/nonlinear_conjugate_gradient.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER_NAME = None\n",
    "if OPTIMIZER_NAME == \"SGD\":\n",
    "    optimizer = torch.optim.SGD() \n",
    "elif OPTIMIZER_NAME == \"FR\":\n",
    "    optimizer = optim.BASIC(\n",
    "        model.parameters(), method = 'FR',\n",
    "        line_search = 'Strong_Wolfe', c1 = 1e-4,\n",
    "        c2 = 0.5, lr = 0.2, max_ls = 25)\n",
    "elif OPTIMIZER_NAME == \"LBFGS\":\n",
    "    optimizer = optimize.BFGS.\n",
    "    \n",
    "def closure(lossfn):\n",
    "    optimizer.zero_grad()\n",
    "    loss = lossfn(model_output, target)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "loss = closure()\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = optimize.lbfgsb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to cut down the number of training samples. It is way too large. Go ahead and remove 30% from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df['label']==10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = train_df[train_df['label']==10]\n",
    "nine = train_df[train_df['label']==9]\n",
    "eight = train_df[train_df['label']==8]\n",
    "seven = train_df[train_df['label']==7]\n",
    "six = train_df[train_df['label']==6]\n",
    "five = train_df[train_df['label']==5]\n",
    "four = train_df[train_df['label']==4]\n",
    "three = train_df[train_df['label']==3]\n",
    "two = train_df[train_df['label']==2]\n",
    "one = train_df[train_df['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = ten.sample(frac = 0.5, replace = False)\n",
    "nine = nine.sample(frac = 0.5, replace = False)\n",
    "eight = eight.sample(frac = 0.5, replace = False)\n",
    "seven = seven.sample(frac = 0.5, replace = False)\n",
    "six = six.sample(frac = 0.5, replace = False)\n",
    "five = five.sample(frac = 0.5, replace = False)\n",
    "four = four.sample(frac = 0.5, replace = False)\n",
    "three = three.sample(frac = 0.5, replace = False)\n",
    "two = two.sample(frac = 0.5, replace = False)\n",
    "one = one.sample(frac = 0.5, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat(objs = [one, two, three, four, five, six, seven, eight, nine, ten], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the NaNs from the train_df then use the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of NaNs in train_df is {train_df.isna().sum()}\")\n",
    "no_nans = train_df.dropna(inplace = False)\n",
    "print(f\"Number of NaNs in no_nans is {no_nans.isna().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = no_nans[no_nans['label']==10]\n",
    "nine = no_nans[no_nans['label']==9]\n",
    "eight = no_nans[no_nans['label']==8]\n",
    "seven = no_nans[no_nans['label']==7]\n",
    "six = no_nans[no_nans['label']==6]\n",
    "five = no_nans[no_nans['label']==5]\n",
    "four = no_nans[no_nans['label']==4]\n",
    "three = no_nans[no_nans['label']==3]\n",
    "two = no_nans[no_nans['label']==2]\n",
    "one = no_nans[no_nans['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST = [one, two, three, four, five, six, seven, eight, nine, ten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in LIST:\n",
    "    print(entry.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten = ten.sample(n = 6000, replace = False)\n",
    "nine = nine.sample(n = 6000, replace = False)\n",
    "eight = eight.sample(n = 6000, replace = False)\n",
    "seven = seven.sample(n = 6000, replace = False)\n",
    "six = six.sample(n = 6000, replace = False)\n",
    "five = five.sample(n = 6000, replace = False)\n",
    "four = four.sample(n = 6000, replace = False)\n",
    "three = three.sample(n = 6000, replace = False)\n",
    "two = two.sample(n = 6000, replace = False)\n",
    "one = one.sample(n = 6000, replace = False)\n",
    "LIST = [one, two, three, four, five, six, seven, eight, nine, ten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in LIST:\n",
    "    print(entry.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat(objs = LIST, ignore_index=True)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.memory_usage(deep=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"Yahoo-Answers-Topic-Classification-Dataset/dataset/yahoo_answers_csv/train_sixty_thousand.csv\", index=False,\n",
    "              header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2 = pd.read_csv(\"Yahoo-Answers-Topic-Classification-Dataset/dataset/yahoo_answers_csv/train_sixty_thousand.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final2['label'][0] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
